[
  {
    "path": "posts/2021-07-10-maximum-likelihood-estimation-an-application-in-r/",
    "title": "Maximum likelihood estimation - an application in R",
    "description": "Taking a gander at the workhorse of statistics and how it can be applied in R.",
    "author": [
      {
        "name": "Rikard Wahlström",
        "url": {}
      }
    ],
    "date": "2021-07-11",
    "categories": [
      "posts"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nSetting the stage\r\nThe data\r\nParametric estimation\r\nAnalytical MLE\r\nNumerical MLE\r\n\r\nConclusion\r\n\r\n\r\n\r\n\r\nIntroduction\r\nMaximum likelihood estimation (MLE) is the workhorse of statistics when it comes to parameter estimation.\r\nIn this post, I will showcase how MLE can be employed in parameter estimation. Using a simple case, this post will showcase both the analytical and the numerical derivation of the maximum likelihood estimates.\r\nSetting the stage\r\nThe data\r\nImagine that the lifespan (denoted as \\(t\\)) of 100 lamps has been measured. Each lamp has been used with a constant intensity (denoted as \\(s\\)) ranging from 0 to 1, where a value of \\(0\\) implies that the lamp is turned off and a value of \\(1\\) implies that the lamp is burning at its highest intensity.\r\nLet’s have a glimpse at the data we are working with.\r\n\r\n\r\nlibrary(tidyverse)\r\ntheme_set(theme_light())\r\n\r\nglimpse(lamp_data)\r\n\r\n\r\nRows: 100\r\nColumns: 2\r\n$ time      <dbl> 2142.03757, 3568.83436, 1020.56774, 103.11813, 102~\r\n$ intensity <dbl> 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.~\r\n\r\nAs stated previously, our sample contains 100 observations (lamps), each of which has been used with a different intensity (ranging from \\(0.01\\) to \\(1.00\\)).\r\nParametric estimation\r\nMaximum likelihood estimation sorts under the umbrella of parametric statistics. This, in simple terms, implies that an assumption regarding the underlying probability distribution of the data has to be made. It is then the unknown parameter or parameters of that assumed probability distribution that is estimated using maximum likelihood-based methods.\r\nFor the present lamp study, let us assume that it is known that lamps of the type relevant here has a lifespan \\(t\\) that is exponentially distributed, that is\r\n\\[f(t) = \\lambda e^{-\\lambda t},\\]\r\nand where the expected value is\r\n\\[\\mu(s) = \\frac{\\beta}{s}, \\quad s > 0,\\]\r\nwhere \\(\\beta > 0\\) is an unknown parameter and \\(s\\) equals the intensity with which the lamp has been used. Our focus of interest is to estimate the unknown parameter \\(\\beta\\).\r\nAnalytical MLE\r\nWe know that the expected value of an exponentially distributed random variable (say \\(t\\)) is given by:\r\n\\(E[t] = \\frac{1}{\\lambda}.\\)\r\nWe also know that, for the relevant sample, the expected value is given by the formula described in the introduction.\r\nWe then have that \\(\\frac{1}{\\lambda} = \\frac{\\beta}{s}.\\)\r\nSolving for \\(\\lambda\\) we get that \\(\\lambda = \\frac{s}{\\beta}.\\)\r\nThus, substituting \\(\\lambda\\) with \\(\\frac{s}{\\beta}\\), the likelihood function for one observation is given by \\(L(\\beta) = \\frac{s}{\\beta}e^{-\\frac{s}{\\beta}t}.\\)\r\nThe likelihood function of the parameter \\(\\beta\\) given all observations is then given by:\r\n\\[\\begin{aligned}\r\nL_n(\\beta) = \\prod_{i = 1}^{n}\\frac{s_i}{\\beta}e^{-\\frac{s_i}{\\beta}t_i}.\r\n\\end{aligned}\\]\r\nAnd the log-likelihood:\r\n\\[\\begin{aligned}\r\n\\ell_n(\\beta) = \r\nlnL_n(\\beta) = \r\nln \\prod_{i = 1}^{n}\\frac{s_i}{\\beta}e^{-\\frac{s_i}{\\beta}t_i} =\r\n\\sum_{i = 1}^n ln \\left[ \\frac{s_i}{\\beta}e^{-\\frac{s_i}{\\beta}t_i} \\right] = \\\\ \r\n\\sum_{i = 1}^n \\left[ ln s_i - ln \\beta - \\frac{1}{\\beta}s_it_i \\right] = \r\n\\sum_{i = 1}^nln s_i - n ln \\beta - \\frac{1}{\\beta}\\sum_{i = 1}^n s_i t_i.\r\n\\end{aligned}\\]\r\nTo calculate the maximum likelihood estimate of \\(\\beta\\), we first take the derivative of \\(\\ell_n(\\beta)\\) w.r.t. to \\(\\beta\\):\r\n\\[\\begin{aligned}\r\n\\frac{d}{d\\beta}\\ell_n(\\beta) = \\ell_n'(\\beta) = \r\n-\\frac{n}{\\beta} + \\frac{1}{\\beta^2}\\sum_{i = 1}^n s_i t_i.\r\n\\end{aligned}\\]\r\nEquating the resulting expression with \\(0\\) and solving for \\(\\beta\\), we get:\r\n\\[\\begin{aligned}\r\n-\\frac{n}{\\beta} + \\frac{1}{\\beta^2}\\sum_{i = 1}^n s_i t_i = 0 \\rightarrow\r\n\\frac{n}{\\beta} = \\frac{1}{\\beta^2}\\sum_{i = 1}^n s_i t_i \\rightarrow\r\n\\hat \\beta = \\frac{1}{n}\\sum_{i = 1}^n s_i t_i,\r\n\\end{aligned}\\]\r\nwhich is then the maximum likelihood estimate of \\(\\beta\\).\r\nNumerical MLE\r\nAbove, we saw how to analytically calculate the maximum likelihood estimate of \\(\\beta\\) by first defining the likelihood function, followed by the log-likelihood function and then taking the first derivative and solving for the parameter of interest.\r\nLet’s implement this in R to get the numerical estimate.\r\n\r\n\r\n# Extract the time column\r\n\r\nt <- lamp_data %>% pull(time)\r\n\r\n# Extract the intensity column\r\n\r\ns <- lamp_data %>% pull(intensity)\r\n\r\n# Get the number of observations\r\n\r\nn <- length(lamp_data$time)\r\n\r\n# Calculate the estimate of beta\r\n\r\n(beta_hat <- round(sum(s*t)/n, 2))\r\n\r\n\r\n[1] 121.43\r\n\r\nThe maximum likelihood estimate is therefore \\(\\hat \\beta =\\) 121.43.\r\nLet’s implement the “entire process” of the above in R (and not just the calculation of the estimate).\r\n\r\n\r\n# Initial calculations\r\n\r\nn <- length(lamp_data$time)\r\n\r\nbeta_hat <- (1/n)*sum(lamp_data$intensity*lamp_data$time)\r\n\r\n# Formulate the log-likelihood function\r\n\r\nlog_ln <- function(beta){\r\n  n <- length(lamp_data$time)\r\n  (sum(log(lamp_data$intensity))) - \r\n    ((n*log(beta))) -\r\n    ((1/beta)*sum(lamp_data$time*lamp_data$intensity))\r\n}\r\n\r\n# Calculating the maximum likelihood estimate\r\n\r\noptim_result <- optim(1.0,\r\n                      log_ln,\r\n                      method = \"Brent\",\r\n                      lower = 0.001,\r\n                      upper = 50000,\r\n                      control = list(fnscale = -1.0))\r\n\r\noptim_result$par\r\n\r\n\r\n[1] 121.4255\r\n\r\nbeta_hat\r\n\r\n\r\n[1] 121.4255\r\n\r\nFrom the above, we see that the result is the same.\r\nBelow is a plot of the observations, together with regression line (calculated from the expected value of \\(t\\), i.e. \\(E[t] = \\frac{\\hat \\beta}{s_i}\\), for each observation):\r\n\r\n\r\nggplot(lamp_data,aes(intensity, time)) +\r\n  geom_point() +\r\n  geom_line(aes(y=t_expected)) +\r\n  labs(x = \"Intensity\",\r\n       y = \"Time\",\r\n       title = \"Regression plot Time ~ Intensity\")\r\n\r\n\r\n\r\n\r\nFrom this, we can see that the ML estimate of \\(\\beta\\) seems reasonable.\r\nConclusion\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-07-10-maximum-likelihood-estimation-an-application-in-r/maximum-likelihood-estimation-an-application-in-r_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2021-07-11T19:22:08+02:00",
    "input_file": "maximum-likelihood-estimation-an-application-in-r.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  }
]
